

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="zh_CN">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Compiling CUDA C/C++ with LLVM &#8212; LLVM 3.8 文档</title>
    <link rel="stylesheet" href="_static/llvm-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '3.8',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/translations.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <link rel="next" title="LLVM Atomic Instructions and Concurrency Guide" href="Atomics.html" />
    <link rel="prev" title="MCJIT Design and Implementation" href="MCJITDesignAndImplementation.html" />
<style type="text/css">
  table.right { float: right; margin-left: 20px; }
  table.right td { border: 1px solid #ccc; }
</style>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-78144609-1', 'auto');
  ga('send', 'pageview');
</script>

  </head>
  <body>
<div class="logo">
  <a href="index.html">
    <img src="_static/logo.png"
         alt="LLVM Logo" width="250" height="88"/></a>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="总目录"
             accesskey="I">索引</a></li>
        <li class="right" >
          <a href="Atomics.html" title="LLVM Atomic Instructions and Concurrency Guide"
             accesskey="N">下一页</a> |</li>
        <li class="right" >
          <a href="MCJITDesignAndImplementation.html" title="MCJIT Design and Implementation"
             accesskey="P">上一页</a> |</li>
  <li><a href="http://llvm.org/">LLVM 官网</a>&nbsp;|&nbsp;</li>
  <li><a href="index.html">LLVM 中文文档</a>&raquo;</li>
 
      </ul>
    </div>


    <div class="document">
      <div class="documentwrapper">
          <div class="body" role="main">
            
  <div class="section" id="compiling-cuda-c-c-with-llvm">
<h1>Compiling CUDA C/C++ with LLVM<a class="headerlink" href="#compiling-cuda-c-c-with-llvm" title="永久链接至标题">¶</a></h1>
<div class="contents local topic" id="id1">
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="id2">Introduction</a></li>
<li><a class="reference internal" href="#how-to-build-llvm-with-cuda-support" id="id3">How to Build LLVM with CUDA Support</a></li>
<li><a class="reference internal" href="#how-to-compile-cuda-c-c-with-llvm" id="id4">How to Compile CUDA C/C++ with LLVM</a></li>
<li><a class="reference internal" href="#optimizations" id="id5">Optimizations</a></li>
</ul>
</div>
<div class="section" id="introduction">
<h2><a class="toc-backref" href="#id2">Introduction</a><a class="headerlink" href="#introduction" title="永久链接至标题">¶</a></h2>
<p>This document contains the user guides and the internals of compiling CUDA
C/C++ with LLVM. It is aimed at both users who want to compile CUDA with LLVM
and developers who want to improve LLVM for GPUs. This document assumes a basic
familiarity with CUDA. Information about CUDA programming can be found in the
<a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA programming guide</a>.</p>
</div>
<div class="section" id="how-to-build-llvm-with-cuda-support">
<h2><a class="toc-backref" href="#id3">How to Build LLVM with CUDA Support</a><a class="headerlink" href="#how-to-build-llvm-with-cuda-support" title="永久链接至标题">¶</a></h2>
<p>Below is a quick summary of downloading and building LLVM. Consult the <a class="reference external" href="http://llvm.org/docs/GettingStarted.html">Getting
Started</a> page for more details on
setting up LLVM.</p>
<ol class="arabic">
<li><p class="first">Checkout LLVM</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="gp">$</span> <span class="nb">cd</span> where-you-want-llvm-to-live
<span class="gp">$</span> svn co http://llvm.org/svn/llvm-project/llvm/trunk llvm
</pre></div>
</div>
</li>
<li><p class="first">Checkout Clang</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="gp">$</span> <span class="nb">cd</span> where-you-want-llvm-to-live
<span class="gp">$</span> <span class="nb">cd</span> llvm/tools
<span class="gp">$</span> svn co http://llvm.org/svn/llvm-project/cfe/trunk clang
</pre></div>
</div>
</li>
<li><p class="first">Configure and build LLVM and Clang</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="gp">$</span> <span class="nb">cd</span> where-you-want-llvm-to-live
<span class="gp">$</span> mkdir build
<span class="gp">$</span> <span class="nb">cd</span> build
<span class="gp">$</span> cmake <span class="o">[</span>options<span class="o">]</span> ..
<span class="gp">$</span> make
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="how-to-compile-cuda-c-c-with-llvm">
<h2><a class="toc-backref" href="#id4">How to Compile CUDA C/C++ with LLVM</a><a class="headerlink" href="#how-to-compile-cuda-c-c-with-llvm" title="永久链接至标题">¶</a></h2>
<p>We assume you have installed the CUDA driver and runtime. Consult the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">NVIDIA
CUDA installation Guide</a> if
you have not.</p>
<p>Suppose you want to compile and run the following CUDA program (<code class="docutils literal"><span class="pre">axpy.cu</span></code>)
which multiplies a <code class="docutils literal"><span class="pre">float</span></code> array by a <code class="docutils literal"><span class="pre">float</span></code> scalar (AXPY).</p>
<div class="highlight-c++"><div class="highlight"><pre><span></span><span class="cp">#include</span> <span class="cpf">&lt;helper_cuda.h&gt; // for checkCudaErrors</span><span class="cp"></span>

<span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp"></span>

<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">axpy</span><span class="p">(</span><span class="kt">float</span> <span class="n">a</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">y</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">argv</span><span class="p">[])</span> <span class="p">{</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">kDataLen</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>

  <span class="kt">float</span> <span class="n">a</span> <span class="o">=</span> <span class="mf">2.0f</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">host_x</span><span class="p">[</span><span class="n">kDataLen</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mf">1.0f</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">,</span> <span class="mf">3.0f</span><span class="p">,</span> <span class="mf">4.0f</span><span class="p">};</span>
  <span class="kt">float</span> <span class="n">host_y</span><span class="p">[</span><span class="n">kDataLen</span><span class="p">];</span>

  <span class="c1">// Copy input data to device.</span>
  <span class="kt">float</span><span class="o">*</span> <span class="n">device_x</span><span class="p">;</span>
  <span class="kt">float</span><span class="o">*</span> <span class="n">device_y</span><span class="p">;</span>
  <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_x</span><span class="p">,</span> <span class="n">kDataLen</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)));</span>
  <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_y</span><span class="p">,</span> <span class="n">kDataLen</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)));</span>
  <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">device_x</span><span class="p">,</span> <span class="n">host_x</span><span class="p">,</span> <span class="n">kDataLen</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
                             <span class="n">cudaMemcpyHostToDevice</span><span class="p">));</span>

  <span class="c1">// Launch the kernel.</span>
  <span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">kDataLen</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">device_x</span><span class="p">,</span> <span class="n">device_y</span><span class="p">);</span>

  <span class="c1">// Copy output data to host.</span>
  <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaDeviceSynchronize</span><span class="p">());</span>
  <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">host_y</span><span class="p">,</span> <span class="n">device_y</span><span class="p">,</span> <span class="n">kDataLen</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
                             <span class="n">cudaMemcpyDeviceToHost</span><span class="p">));</span>

  <span class="c1">// Print the results.</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">kDataLen</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;y[&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">i</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;] = &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">host_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaDeviceReset</span><span class="p">());</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The command line for compilation is similar to what you would use for C++.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="gp">$</span> clang++ -o axpy -I&lt;CUDA install path&gt;/samples/common/inc -L&lt;CUDA install path&gt;/&lt;lib64 or lib&gt; axpy.cu -lcudart_static -lcuda -ldl -lrt -pthread
<span class="gp">$</span> ./axpy
<span class="go">y[0] = 2</span>
<span class="go">y[1] = 4</span>
<span class="go">y[2] = 6</span>
<span class="go">y[3] = 8</span>
</pre></div>
</div>
<p>Note that <code class="docutils literal"><span class="pre">helper_cuda.h</span></code> comes from the CUDA samples, so you need the
samples installed for this example. <code class="docutils literal"><span class="pre">&lt;CUDA</span> <span class="pre">install</span> <span class="pre">path&gt;</span></code> is the root
directory where you installed CUDA SDK, typically <code class="docutils literal"><span class="pre">/usr/local/cuda</span></code>.</p>
</div>
<div class="section" id="optimizations">
<h2><a class="toc-backref" href="#id5">Optimizations</a><a class="headerlink" href="#optimizations" title="永久链接至标题">¶</a></h2>
<p>CPU and GPU have different design philosophies and architectures. For example, a
typical CPU has branch prediction, out-of-order execution, and is superscalar,
whereas a typical GPU has none of these. Due to such differences, an
optimization pipeline well-tuned for CPUs may be not suitable for GPUs.</p>
<p>LLVM performs several general and CUDA-specific optimizations for GPUs. The
list below shows some of the more important optimizations for GPUs. Most of
them have been upstreamed to <code class="docutils literal"><span class="pre">lib/Transforms/Scalar</span></code> and
<code class="docutils literal"><span class="pre">lib/Target/NVPTX</span></code>. A few of them have not been upstreamed due to lack of a
customizable target-independent optimization pipeline.</p>
<ul class="simple">
<li><strong>Straight-line scalar optimizations</strong>. These optimizations reduce redundancy
in straight-line code. Details can be found in the <a class="reference external" href="https://goo.gl/4Rb9As">design document for
straight-line scalar optimizations</a>.</li>
<li><strong>Inferring memory spaces</strong>. <a class="reference external" href="http://www.llvm.org/docs/doxygen/html/NVPTXFavorNonGenericAddrSpaces_8cpp_source.html">This optimization</a>
infers the memory space of an address so that the backend can emit faster
special loads and stores from it. Details can be found in the <a class="reference external" href="https://goo.gl/5wH2Ct">design
document for memory space inference</a>.</li>
<li><strong>Aggressive loop unrooling and function inlining</strong>. Loop unrolling and
function inlining need to be more aggressive for GPUs than for CPUs because
control flow transfer in GPU is more expensive. They also promote other
optimizations such as constant propagation and SROA which sometimes speed up
code by over 10x. An empirical inline threshold for GPUs is 1100. This
configuration has yet to be upstreamed with a target-specific optimization
pipeline. LLVM also provides <a class="reference external" href="http://clang.llvm.org/docs/AttributeReference.html#pragma-unroll-pragma-nounroll">loop unrolling pragmas</a>
and <code class="docutils literal"><span class="pre">__attribute__((always_inline))</span></code> for programmers to force unrolling and
inling.</li>
<li><strong>Aggressive speculative execution</strong>. <a class="reference external" href="http://llvm.org/docs/doxygen/html/SpeculativeExecution_8cpp_source.html">This transformation</a> is
mainly for promoting straight-line scalar optimizations which are most
effective on code along dominator paths.</li>
<li><strong>Memory-space alias analysis</strong>. <a class="reference external" href="http://reviews.llvm.org/D12414">This alias analysis</a> infers that two pointers in different
special memory spaces do not alias. It has yet to be integrated to the new
alias analysis infrastructure; the new infrastructure does not run
target-specific alias analysis.</li>
<li><strong>Bypassing 64-bit divides</strong>. <a class="reference external" href="http://llvm.org/docs/doxygen/html/BypassSlowDivision_8cpp_source.html">An existing optimization</a>
enabled in the NVPTX backend. 64-bit integer divides are much slower than
32-bit ones on NVIDIA GPUs due to lack of a divide unit. Many of the 64-bit
divides in our benchmarks have a divisor and dividend which fit in 32-bits at
runtime. This optimization provides a fast path for this common case.</li>
</ul>
</div>
</div>


          </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="总目录"
             >索引</a></li>
        <li class="right" >
          <a href="Atomics.html" title="LLVM Atomic Instructions and Concurrency Guide"
             >下一页</a> |</li>
        <li class="right" >
          <a href="MCJITDesignAndImplementation.html" title="MCJIT Design and Implementation"
             >上一页</a> |</li>
  <li><a href="http://llvm.org/">LLVM 官网</a>&nbsp;|&nbsp;</li>
  <li><a href="index.html">LLVM 中文文档</a>&raquo;</li>
 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2003-2018, LLVM Project.
      最后更新于 2018-01-13.
      由 <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.6 创建。
    </div>
  </body>
</html>